%\documentclass{article}
%\usepackage[letterpaper,margin=2.1cm]{geometry}
%\usepackage{xcolor}
%\usepackage{fancyhdr}
%\usepackage{tgschola} % or any other font package you like

\documentclass[12pt]{article}
\usepackage{extsizes}
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsxtra}
\usepackage{wasysym}
\usepackage{isomath}
\usepackage{mathtools}
\usepackage{txfonts}
\usepackage{upgreek}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{tensor}
\usepackage{pifont}
\usepackage[margin=15mm]{geometry}
\definecolor{color-1}{rgb}{0.26,0.26,0.26}
\definecolor{color-2}{rgb}{0.4,0.4,0.4}
\usepackage{extsizes}
\usepackage{tocbibind}
\usepackage{float}
\usepackage{flafter}
\usepackage{xcolor}
\usepackage{sectsty}
\usepackage[font=small, skip=0pt]{caption}
\usepackage{setspace}
\setstretch{1.1}
\usepackage{fancyhdr}

\usepackage{nopageno}

% Select the font
\usepackage{charter}


\usepackage[%
square,        % for square brackets
comma,         % use commas as separators
numbers,       % for numerical citations;
%sort           % orders multiple citations into the sequence in which they appear in the list of references;
sort&compress % as sort but in addition multiple numerical citations
% are compressed if possible (as 3-6, 15);
]{natbib}

\renewcommand{\bibfont}{\normalfont\footnotesize}



\usepackage{hyperref}
\hypersetup{
	colorlinks = true,
	citecolor = {blue},
	linkcolor = {blue},
	urlcolor  = {blue},
}






\newcommand{\soptitle}{Intelligent Mobile Edge Computing Systems with Reinforcement Learning-Based Innovations}
\newcommand{\yourname}{Iman Rahmati}
\newcommand{\youremail}{iman.rahmati@sharif.edu}
\newcommand{\yourweb}{\href{https://imanrht.github.io}{imanrht.github.io}}

\newcommand{\statement}[1]{\par\medskip
	\underline{\textcolor{blue}{\textbf{#1:}}}\space
}

%\usepackage[
%colorlinks,
%breaklinks,
%pdftitle={\yourname - \soptitle},
%pdfauthor={\yourname},
%urlcolor  = blue,
%citecolor = blue,
%anchorcolor = blue,
%unicode
%]{hyperref}


\usepackage{setspace}
\onehalfspacing

\begin{document}
	

	
%\pagestyle{fancy}
%\fancyhf{}
%\fancyhead[C]{%
%	\footnotesize\sffamily\vspace{8mm}
%	\textcolor{blue}{\href{mailto:iman.rahmati@sharif.edu}{Research Ideas, V0.1}}  \hfill
%	\textcolor{blue}{\href{https://imanrht.github.io/assets/images/CV_ImanRahmati.pdf}{20 Sep. 2024\vspace{2mm}}}}
%


\begin{center} 
	
		\vspace{-17mm}
	
	\large Iman Rahmati\footnote{The author is with the EdgeAI Labratory at Department of Electrical Engineering, Sharif University of Technology, Tehran, Iran (\href{mailto:iman.rahmati@sharif.edu}{Mail}, \href{https://github.com/ImanRHT}{Github})} \hfill Research Ideas. V0.1, 20 Sep. 2024 \vspace{1mm} \hrule
	
	\vspace{-1mm}
	
	
	\noindent\Large 
	\textcolor{white}{i} \\ \LARGE\soptitle \vspace{6mm}\\
	%\Large\yourname \large
\end{center}
	
	

%	\hrule
%	\vspace{0.5pt}
%	\hrule height 1pt



\vspace{-8mm}

\begin{abstract}
	\noindent
	Mobile edge computing often suffers from the dynamic and unknown nature of the environment such as time-varying conditions, heterogeneous devices, and frequent communication requests, imposing significant challenges on improving system performance. To meet the rapidly growing demands of computation-intensive and time-sensitive applications, Reinforcement learning (RL)\cite{mnih2015human} has been proposed as an effective tool to establish low-latency and energy-efficient networks. RL enables network entities to interact with the environment and learn an optimal decision-making policy, usually modeled as a Markov decision process (MDP)\cite{puterman2014markov}.\vspace{2.5mm}
	
	 \noindent\textit{\textbf{Termsâ€”}} Mobile edge computing (MEC), Resource Management, Computation Offloading, Partially Observeble MDP (POMDP), Deep RL (DRL), Multi-Agent DRL (MADRL), Meta-RL, Federated RL
\end{abstract}




	
\vspace{0mm}
\noindent\large\textbf{Introduction: }
\small
MEC is emerging as a promising paradigm to enhance the computational capacity of mobile devices by offloading tasks to nearby edge servers. This paradigm aims to reduce latency, energy consumption, and improve quality of experience for end-users. However, one of the major challenges in MEC is the efficient decision-making process for resource management, considering the dynamic nature of the network, user demands, and limited resources. Traditional offloading strategies, which often rely on heuristic or single-agent models, fail to capture the complexity and stochastic nature of modern MEC. \\\vspace{-2mm}

\noindent\large\textbf{Background: }\small
To cope with the dynamic nature of the network, state-of-the-art researchs has proposed several task offloading algorithms using RL-based methods, which hold promises to determine optimal decision-making policies by capturing the dynamics of environments and learning strategies for accomplishing long-term objectives \cite{arulkumaran2017deep}. Munir et al. introduced a semi-distributed multi-agent reinforcement learning (MARL) method for self-powered MEC. Gong et al. proposed a deep reinforcement learning (DRL) structure for task offloading and resource allocation, aiming to reduce energy use and task delays. Wu et al. addressed privacy in computation offloading using a deep Q-network (DQN) to optimize energy and computation rates in queuing systems. Lastly, Sun et al. tackled computation offloading and service caching with a hierarchical DRL framework to minimize long-term service delays across diverse resources.

 	Although DRL-based methods have demonstrated their effectiveness in handling network dynamics, task offloading still encounters several challenges that require further attention: 

 	\begin{enumerate}
 		\item\textbf{\hspace{2mm}Devise-edge task offloading.}  
 		\item\textbf{\hspace{2mm}Edge-edge task offloading.} 
 		\item\textbf{\hspace{2mm}Edge-edge task offloading.}
 	\end{enumerate}

\newpage

The rapid growth of mobile applications, such as augmented reality, real-time gaming, and high-definition video streaming, has placed significant demands on the computational resources of mobile devices. While the processing power of these devices has improved, there remain significant limitations in battery life, processing speed, and memory capacity. Mobile Edge Computing (MEC) has emerged as a solution, enabling computation offloading to nearby edge servers to alleviate the processing burden on mobile devices. However, the challenge lies in determining how and when to offload computational tasks efficiently, especially in a dynamic network environment with varying resources and user demands. The decision-making process becomes more complex with the presence of multiple devices competing for limited edge resources. Current solutions often employ heuristic or single-agent approaches, which are not robust in highly dynamic and multi-user MEC environments. Multi-Agent Deep Reinforcement Learning (DRL) offers a promising avenue for addressing this challenge. By allowing multiple agents (mobile devices and edge servers) to autonomously learn and adapt their offloading strategies, it becomes possible to optimize system-wide performance metrics such as energy consumption, latency, and resource utilization. 




 This ultimately improves the MD users' QoE. We explore state-of-the-art RL-based techniques characteristic to discuss how they address task offloading problems in MEC. 
 
 To minimize energy consumption, Munir \textit{et al.} \cite{munir2021multi} developed a semi-distributed approach using a multi-agent RL (MARL) framework for self-powered MEC. Gong \textit{et al.} in \cite{gong2022edge} proposed a DRL-based network structure in the industrial IoT (IIoT) systems to jointly optimize task offloading and resource allocation to achieve lower energy consumption and decreased task delay. To investigate privacy aware computation offloading problem, Wu \textit{et al.} in \cite{wu2024combining} proposed a deep Q-network (DQN)-based method. They transform the problem on MDP to optimize computation rate and energy consumption in a queuing-based IIoT network. Sun \textit{et al.} in \cite{sun2024hierarchical} explored both computation offloading and service caching problems in MEC. They formulated an optimization problem that aims to minimize the long-term average service delay. They then proposed a hierarchical DRL framework, which effectively handles both problems under heterogeneous resources
 

\noindent\large\textbf{Problems}

\newpage






\begin{center} 
	
	
	\vspace{-17mm}

	\large Research Idea. 1  \hfill Multi-Agent DRL \vspace{1mm} \hrule
	
	\vspace{-1mm}
	
	\textcolor{white}{i} \\ \LARGE Multi-Agent Deep Reinforcement Learning for Cooperative Task Offloading in Partially Observable Mobile Edge Computing Environment \vspace{6mm}\\ 

\end{center} \small
\vspace{-5mm}

\noindent\textbf{\large Motivation:  }
\noindent
In MEC, each entity may need to make local decisions to improve network performance in dynamic and uncertain environments. Standard learning algorithms, such as single-agent Reinforcement Learning (RL) or Deep Reinforcement Learning (DRL), have recently been used to enable each network entity to learn an optimal decision-making policy adaptively through interaction with the unknown environment. However, these algorithms fail to model cooperation or competition among network entities, treating other entities simply as part of the environment, which can lead to non-stationarity issues. Multi-Agent Reinforcement Learning (MARL) enables each network entity to learn its optimal policy by observing both the environment and the policies of other entities while interacting with a shared or separate environment to achieve specific objectives.
 
 %As a result, MARL can significantly improve the learning efficiency of the network entities, and it has been recently used to solve various issues in the emerging networks.


%In multi-agent DRL, multiple agents interact with a shared or separate environment to achieve specific objectives. Each agent independently learns through trial and error while accounting for the actions and policies of other agents. Multiple agents interact with a shared or separate environment to achieve specific objectives. In mobile edge computing, each device might be an agent trying to optimize its own computation offloading strategy while considering the resource usage and strategies of other devices.


\vspace{2mm}

\noindent\textbf{\large Problem Statement: }
\noindent
Task offloading is a critical process to efficiently assign available resources to task requests, for high-performance, reliable, and cost-effective services [], []. In the MEC, the task offloading decision-making process focuses on efficiently distributing tasks among edge servers, where resources refer to limited computation, storage, and communication resources of edge and cloud servers. Typically, the offloading proceses involves two layers of heterogeneous decisions making prolems (\textbf{P1, P2}) as follow,
%The task offloading decision-making process focuses on efficiently distributing tasks among edge servers. 
\vspace{-2mm}
\begin{itemize}
	\item\textbf{P1.\hspace{2mm}Devise-edge task offloading.} Enables devices to independently make decisions on offloading resource-intensive tasks to nearby edge servers, fostering efficient utilization of resources.\vspace{-2mm}
	\item\textbf{P2.\hspace{2mm}Edge-edge task offloading.} Leverages edge-edge collaborations, where tasks initially received by a local edge server can be offloaded to neighboring servers with underutilized resources, ensuring better resource utilization. %Offloading tasks between edge servers requires communication resources and may introduce additional transmission delay, which should be taken into account when designing offloading strategies.
\end{itemize}

\noindent\textbf{\large Problem Model: }
\noindent
The main problem can be formoulated as decomposation of sub-problems \textbf{P1} and \textbf{P2} as a \textbf{Decenteralized Partially Observable Markove Decision Procsses (Dec-POMDP}), where multibe devices and edge servers interacting with each other by its own observation of environment, which is a part of main overall state. 

\vspace{3mm}

\noindent\textbf{\large Research Methodology: }
\begin{enumerate} 
	\item \textbf{Algorithm Design:} Developing a Multi-Agent Deep Reinforcement Learning algorithm using techniques such as \textbf{Deep Deterministic Policy Gradiant (DDPG)} or \textbf{Dueling Double Deep Q-Networks (D3QN)}, with a focus on communication and collaboration, cordination or competitation between agents. \vspace{-1mm}
	\item \textbf{Simulation Environment:} A simulated MEC environment will be developed using Python or a suitable simulation platform, where mobile devices can offload tasks to edge servers and edge servers can distribute thier computation workloads, under different network conditions.\vspace{-1mm}
	\item \textbf{Key Challenges:} (a) Coordination or competition between agents. (b) Non-stationary environment due to actions of other agents.  (c) Scalability issues as the number of agents increases.
\end{enumerate}



\newpage




\begin{center} 
	
	
	\vspace{-17mm}
	
	\large Research Idea. 2  \hfill Multi-Agent Meta-RL \vspace{1mm} \hrule
	
	\vspace{-1mm}
	
	
	
	\textcolor{white}{i} \\ \LARGE Meta-Reinforcement Learning for Optimized Resource Management in Heterogeneous Mobile Edge Computing  \vspace{6mm}\\
	
\end{center}
\vspace{-5mm}

\noindent\textbf{\large Motivation:  }
\noindent
Meta DRL focuses on training agents that can quickly adapt to new tasks or environments with minimal additional learning. It is designed for scenarios where agents face a wide variety of tasks, and the aim is to learn a policy that generalizes well across different tasks. The primary objective is to equip the agent with meta-knowledge, allowing it to efficiently adapt to new tasks by leveraging past learning experiences. In MEC, a meta-trained agent could adapt its offloading strategy efficiently when moving between different environments (e.g., from urban to rural networks), quickly optimizing its offloading decisions in unfamiliar settings.


\vspace{3mm}
 
 

\noindent\textbf{\large Problem Statement: } Efficient task offloading is crucial to ensure seamless resource distribution in MEC.
Typically, the overall Resource Management process involves three layers of heterogeneous Resource scheduling decisions (\textbf{P1}, \textbf{P2}, \textbf{P3}), each of which performs in a specific collaboration manner. \vspace{-2mm}

\begin{itemize}
	
	\item \textbf{P1. Edge-cloud service placement. } %In MEC systems, clouds have sufficient resources, while edge servers are resource-limited. 
	The cloud caches all services with sufficient storage spaces. Considering storage limits of edge servers, only a subset of services can be placed in each edge server. Services can be migrated from a cloud to an edge or between edge servers, which requires efficient collaboration.\vspace{-2mm}
	
	\item\textbf{P2. Edge-edge computation offloading. }  The task offloading decision-making process focuses on efficiently distributing tasks among edge servers. Edge-edge collaborations enables edge servers to offloade their computation workload to neighboring servers, ensuring better resource utilization. \vspace{-2mm}
	
	\item\textbf{P3. Intra-edge resource allocation. } On edge servers, there may be several tasks competing for resources among offloaded tasks on the same server. Intra edge there is a resource competition among offloaded tasks on the same server. Intra-edge resource allocation aims to determine how resources should be allocated to each offloaded task.
	
\end{itemize}

\vspace{0mm}

\noindent\textbf{\large Problem Model: } To apply Meta-RL for address combination of sub-problems  \textbf{P1},  \textbf{P2}, and \textbf{P3}, each problems can be formoulated as an individual MDP models. The MDP learning process shoud be decomposed into two parts: \textbf{learning a meta policy efficiently across all MDPs} and \textbf{learning a specific strategy for an MDP quickly based on the learned meta policy}.
\noindent

\vspace{5mm}

\noindent\textbf{\large Research Methodology:}

\begin{enumerate}  \item \textbf{Algorithm Design:} Developing a Multi-Agent Meta-Reinforcement Learning algorithm using techniques such as \textbf{Multi-Agent Meta-Actor and Meta-Critic Networks}, with a focus on global optimization in MEC.\vspace{-1mm}

\item \textbf{Simulation Environment:} A simulated MEC environment will be developed using Python or a suitable simulation platform, where cload and edge servers be able to cache services and distribut tasks in whole resources, under different network conditions. 
	
\item \textbf{Key Challenges:}  (a) The meta-learned policy should work well across different, unseen tasks. (b) Balancing between exploration (learning new tasks) and exploitation (using learned knowledge). 
\end{enumerate}


\newpage



\begin{center} 
	
	
	\vspace{-17mm}
	
	\large Research Idea. 3  \hfill Federated DRL \vspace{1mm} \hrule
	
	\vspace{-1mm}
	
	
	
	
	\textcolor{white}{i} \\ \LARGE Federated Deep Reinforcement Learning for Continuous Improving Intradependente Task Offloading in Mobile Edge Computing Network\vspace{6mm}\\
	
\end{center}
\vspace{-5mm}

\noindent\textbf{\large Motivation:  } 
Federated Reinforcement Learning extends traditional DRL by allowing multiple agents to collaboratively learn a global policy without sharing their local data directly. Each agent makes decisions based on its local observations while cooperating with others to achieve shared system goals. In a Mobile Edge Computing (MEC) environment, edge devices can independently train DRL models using their local data and periodically send updates to a central server. The server aggregates these updates to build a global model, optimizing task offloading across the network. Specificly, federated DRL focuses on collaborative learning across decentralized devices while preserving data privacy. 
\vspace{-2mm}
\begin{itemize}

		\item \textbf{Accelerate training process} for agents involved in MEC network. \vspace{-2mm}
		\item Enable mobile devices to  \textbf{collectively contribute to enhancing the offloading model}.\vspace{-2mm}
		\item Support  \textbf{continuous learning} as new mobile devices join the network.\vspace{-2mm}
\end{itemize}

\vspace{3mm}

\noindent\textbf{\large Problem Statement: }
Efficient task offloading is crucial for optimizing resource utilization and minimizing latency. However, existing approaches often treat tasks as independent, overlooking the complexities introduced by intra-dependencies among tasks, leading to suboptimal resource utilization and performance. To address Dependency-Aware Task Offloading, there is a need for effectively model intra-dependent tasks,
\vspace{-1mm}
	\begin{itemize}
	\item \textbf{Dependency-Aware Task Offloading}\vspace{-2mm}
	\begin{itemize}
		\item Incorporate a \textbf{Task Call Graph Representation} to account for dependencies among tasks, improving task model accuracy and offloading effectiveness. Task call graphs can effectively represent dependencies, allowing for a clearer understanding of task relationships. 
	\end{itemize}\vspace{-3mm}


\end{itemize}

\vspace{3mm}

\noindent\textbf{\large Problem Model: } 
\noindent



\vspace{5mm}

\noindent\textbf{\large Research Methodology}

\begin{enumerate} \item \textbf{Algorithm Design:} Developing a  \textbf{federated DRL} framework for optimizing interdependent task offloading in MEC networks, using teqnics such as \textbf{D3QN},  \textbf{DDPG}, or  \textbf{Actor-Critic Network}, which enable network entities to collaboratively learn and adapt offloading strategies without compromising user privacy. \item \textbf{Simulation Environment:} A simulated MEC environment will be developed using Python or a suitable simulation platform, where devices be able to dispatch their tasks to edge servers and provides collabration for all devices and edge servers, under different network conditions. 
	
	
	\item \textbf{Key Challenges:} (a) Communication efficiency to optimize data exchange between devices and servers.  (b) Heterogeneity to accommodate diverse network components and devices.
\end{enumerate}



\section{Summary of Differences} \begin{itemize} \item \textbf{Multi-Agent DRL} focuses on interactions between multiple agents, each learning in the presence of others. \item \textbf{Federated DRL} focuses on collaborative learning across decentralized devices while preserving data privacy. \item \textbf{Meta DRL} focuses on quick adaptation to new tasks by using past learning experiences. \end{itemize}




\bibliographystyle{IEEEtranN} % IEEEtranN is the natbib compatible bst file
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{paper}




\end{document}


