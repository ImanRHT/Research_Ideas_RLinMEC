%\documentclass{article}
%\usepackage[letterpaper,margin=2.1cm]{geometry}
%\usepackage{xcolor}
%\usepackage{fancyhdr}
%\usepackage{tgschola} % or any other font package you like

\documentclass[12pt]{article}
\usepackage{extsizes}
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsxtra}
\usepackage{wasysym}
\usepackage{isomath}
\usepackage{mathtools}
\usepackage{txfonts}
\usepackage{upgreek}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{tensor}
\usepackage{pifont}
\usepackage[margin=15mm]{geometry}
\definecolor{color-1}{rgb}{0.26,0.26,0.26}
\definecolor{color-2}{rgb}{0.4,0.4,0.4}
\usepackage{extsizes}
\usepackage{tocbibind}
\usepackage{float}
\usepackage{flafter}
\usepackage{xcolor}
\usepackage{sectsty}
\usepackage[font=small, skip=0pt]{caption}
\usepackage{setspace}
\setstretch{1.1}
\usepackage{fancyhdr}

\usepackage{nopageno}

% Select the font
\usepackage{charter}


\usepackage[%
square,        % for square brackets
comma,         % use commas as separators
numbers,       % for numerical citations;
%sort           % orders multiple citations into the sequence in which they appear in the list of references;
sort&compress % as sort but in addition multiple numerical citations
% are compressed if possible (as 3-6, 15);
]{natbib}

\renewcommand{\bibfont}{\normalfont\footnotesize}
\usepackage{hyperref}
\hypersetup{
	colorlinks = true,
	citecolor = {blue}
}







\newcommand{\soptitle}{Reinforcement Learning Improves Edge Computing}
\newcommand{\yourname}{Iman Rahmati}
\newcommand{\youremail}{iman.rahmati@sharif.edu}
\newcommand{\yourweb}{\href{https://imanrht.github.io}{imanrht.github.io}}

\newcommand{\statement}[1]{\par\medskip
	\underline{\textcolor{blue}{\textbf{#1:}}}\space
}

%\usepackage[
%colorlinks,
%breaklinks,
%pdftitle={\yourname - \soptitle},
%pdfauthor={\yourname},
%urlcolor  = blue,
%citecolor = blue,
%anchorcolor = blue,
%unicode
%]{hyperref}


\usepackage{setspace}
\onehalfspacing

\begin{document}
	

	
%\pagestyle{fancy}
%\fancyhf{}
%\fancyhead[C]{%
%	\footnotesize\sffamily\vspace{8mm}
%	\textcolor{blue}{\href{mailto:iman.rahmati@sharif.edu}{Research Ideas, V0.1}}  \hfill
%	\textcolor{blue}{\href{https://imanrht.github.io/assets/images/CV_ImanRahmati.pdf}{20 Sep. 2024\vspace{2mm}}}}
%


\begin{center} 
	
		\vspace{-17mm}
	
	\large Iman Rahmati  \hfill Research Ideas. V0.1, 20 Sep. 2024 \vspace{1mm} \hrule
	
	\vspace{-1mm}
	
	\textcolor{white}{i} \\ \LARGE\soptitle \vspace{6mm}\\
	%\Large\yourname \large
\end{center}
	
%	\hrule
%	\vspace{0.5pt}
%	\hrule height 1pt



\vspace{-8mm}

\begin{abstract}
	\vspace{-2mm}
	\noindent
	Mobile edge computing often suffers from the dynamic and unknown nature of the environment such as time-varying conditions, heterogeneous devices, and frequent communication requests, imposing significant challenges on improving system performance. To meet the rapidly growing demands of computation-intensive and time-sensitive applications, Reinforcement learning \cite{mnih2015human} has been proposed as an effective tool to establish low-latency and energy-efficient networks. RL enables network entities to interact with the environment and learn an optimal decision-making policy, usually modeled as a Markov decision process \cite{puterman2014markov}.
\end{abstract}



	
\vspace{4mm}

\noindent\large\textbf{Introduction}

\vspace{1.5mm}
\normalsize

Mobile Edge Computing is emerging as a promising paradigm to enhance the computational capacity of mobile devices by offloading tasks to nearby edge servers. This paradigm aims to reduce latency, energy consumption, and improve Quality of Experience (QoE) for end-users. However, one of the major challenges in MEC is the efficient decision-making process for computation offloading, considering the dynamic nature of the network, user demands, and limited resources. Traditional offloading strategies, which often rely on heuristic or single-agent models, fail to capture the com- plexity and stochastic nature of modern MEC systems. 


	
\vspace{8mm}

\noindent\textbf{\large Motivation}

\vspace{1.5mm}

The rapid growth of mobile applications, such as augmented reality, real-time gaming, and high-definition video streaming, has placed significant demands on the computational resources of mobile devices. While the processing power of these devices has improved, there remain significant limitations in battery life, processing speed, and memory capacity. Mobile Edge Computing (MEC) has emerged as a solution, enabling computation offloading to nearby edge servers to alleviate the processing burden on mobile devices. However, the challenge lies in determining how and when to offload computational tasks efficiently, especially in a dynamic network environment with varying resources and user demands. The decision-making process becomes more complex with the presence of multiple devices competing for limited edge resources. Current solutions often employ heuristic or single-agent approaches, which are not robust in highly dynamic and multi-user MEC environments. Multi-Agent Deep Reinforcement Learning (DRL) offers a promising avenue for addressing this challenge. By allowing multiple agents (mobile devices and edge servers) to autonomously learn and adapt their offloading strategies, it becomes possible to optimize system-wide performance metrics such as energy consumption, latency, and resource utilization. 

\newpage






\begin{center} 
	
	
	\vspace{-17mm}

	\large Research Idea. 1  \hfill Multi-Agent DRL \vspace{1mm} \hrule
	
	\vspace{-1mm}
	
	\textcolor{white}{i} \\ \LARGE Multi-Agent Deep Reinforcement Learning for Cooperative Task Offloading in Partially Observable Mobile Edge Computing Environment \vspace{6mm}\\ 

\end{center} \small
\vspace{-5mm}

\noindent\textbf{\large Motivation:  }
\noindent
In MEC, each entity may need to make local decisions to improve network performance in dynamic and uncertain environments. Standard learning algorithms, such as single-agent Reinforcement Learning (RL) or Deep Reinforcement Learning (DRL), have recently been used to enable each network entity to learn an optimal decision-making policy adaptively through interaction with the unknown environment. However, these algorithms fail to model cooperation or competition among network entities, treating other entities simply as part of the environment, which can lead to non-stationarity issues. Multi-Agent Reinforcement Learning (MARL) enables each network entity to learn its optimal policy by observing both the environment and the policies of other entities while interacting with a shared or separate environment to achieve specific objectives.
 
 %As a result, MARL can significantly improve the learning efficiency of the network entities, and it has been recently used to solve various issues in the emerging networks.


%In multi-agent DRL, multiple agents interact with a shared or separate environment to achieve specific objectives. Each agent independently learns through trial and error while accounting for the actions and policies of other agents. Multiple agents interact with a shared or separate environment to achieve specific objectives. In mobile edge computing, each device might be an agent trying to optimize its own computation offloading strategy while considering the resource usage and strategies of other devices.


\vspace{2mm}

\noindent\textbf{\large Problem Statement: }
\noindent
Task offloading is a critical process to efficiently assign available resources to task requests, for high-performance, reliable, and cost-effective services [], []. In the MEC, the task offloading decision-making process focuses on efficiently distributing tasks among edge servers, where resources refer to limited computation, storage, and communication resources of edge and cloud servers. Typically, the offloading proceses involves two layers of heterogeneous decisions making prolems (\textbf{P1, P2}) as follow,
%The task offloading decision-making process focuses on efficiently distributing tasks among edge servers. 
\vspace{-2mm}
\begin{itemize}
	\item\textbf{P1.\hspace{2mm}Devise-edge task offloading.} Enables devices to independently make decisions on offloading resource-intensive tasks to nearby edge servers, fostering efficient utilization of resources.\vspace{-2mm}
	\item\textbf{P2.\hspace{2mm}Edge-edge task offloading.} Leverages edge-edge collaborations, where tasks initially received by a local edge server can be offloaded to neighboring servers with underutilized resources, ensuring better resource utilization. %Offloading tasks between edge servers requires communication resources and may introduce additional transmission delay, which should be taken into account when designing offloading strategies.
\end{itemize}

\noindent\textbf{\large Problem Model: }
\noindent
The main problem can be formoulated as decomposation of sub-problems \textbf{P1} and \textbf{P2} as a \textbf{Decenteralized partially observable markove decision procsses (Dec-POMDP}), where multibe devices and edge servers interacting with each other by its own observation of environment, which is a part of main overall state. 

\vspace{3mm}

\noindent\textbf{\large Research Methodology: }
\begin{enumerate} 
	\item \textbf{Algorithm Design:} Developing a Multi-Agent Deep Reinforcement Learning algorithm using techniques such as \textbf{Deep Deterministic Policy Gradiant (DDPG)} or \textbf{Dueling Deep Q-Networks (DDQN)}, with a focus on communication and collaboration, cordination or competitation between agents. \vspace{-1mm}
	\item \textbf{Simulation Environment:} A simulated MEC environment will be developed using Python or a suitable simulation platform, where mobile devices can offload tasks to edge servers and edge servers can distribute thier computation workloads, under different network conditions.\vspace{-1mm}
	\item \textbf{Key Challenges:} (a) Coordination or competition between agents. (b) Non-stationary environment due to actions of other agents.  (c) Scalability issues as the number of agents increases.
\end{enumerate}



\newpage




\begin{center} 
	
	
	\vspace{-17mm}
	
	\large Research Idea. 2  \hfill Multi-Agent Meta-RL \vspace{1mm} \hrule
	
	\vspace{-1mm}
	
	
	
	\textcolor{white}{i} \\ \LARGE Meta-Reinforcement Learning for Optimized Resource Management in Heterogeneous Mobile Edge Computing  \vspace{6mm}\\
	
\end{center}
\vspace{-5mm}

\noindent\textbf{\large Motivation:  }
\noindent
Meta DRL focuses on training agents that can quickly adapt to new tasks or environments with minimal additional learning. It is designed for scenarios where agents face a wide variety of tasks, and the aim is to learn a policy that generalizes well across different tasks. The primary objective is to equip the agent with meta-knowledge, allowing it to efficiently adapt to new tasks by leveraging past learning experiences. In MEC, a meta-trained agent could adapt its offloading strategy efficiently when moving between different environments (e.g., from urban to rural networks), quickly optimizing its offloading decisions in unfamiliar settings.


\vspace{3mm}
 
 

\noindent\textbf{\large Problem Statement: } Efficient task offloading is crucial to ensure seamless resource distribution in MEC.
Typically, the overall Resource Management process involves three layers of heterogeneous Resource scheduling decisions (\textbf{P1}, \textbf{P2}, \textbf{P3}), each of which performs in a specific collaboration manner. \vspace{-2mm}

\begin{itemize}
	
	\item \textbf{P1. Edge-cloud service placement. } %In MEC systems, clouds have sufficient resources, while edge servers are resource-limited. 
	The cloud caches all services with sufficient storage spaces. Considering storage limits of edge servers, only a subset of services can be placed in each edge server. Services can be migrated from a cloud to an edge or between edge servers, which requires efficient collaboration.\vspace{-2mm}
	
	\item\textbf{P2. Edge-edge computation offloading. }  The task offloading decision-making process focuses on efficiently distributing tasks among edge servers. Edge-edge collaborations enables edge servers to offloade their computation workload to neighboring servers, ensuring better resource utilization. \vspace{-2mm}
	
	\item\textbf{P3. Intra-edge resource allocation. } On edge servers, there may be several tasks competing for resources among offloaded tasks on the same server. Intra edge there is a resource competition among offloaded tasks on the same server. Intra-edge resource allocation aims to determine how resources should be allocated to each offloaded task.
	
\end{itemize}

\vspace{0mm}

\noindent\textbf{\large Problem Model: } To apply Meta-RL for address combination of sub-problems  \textbf{P1},  \textbf{P2}, and \textbf{P3}, each problems can be formoulated as an individual MDP models. The MDP learning process shoud be decomposed into two parts: \textbf{learning a meta policy efficiently across all MDPs} and \textbf{learning a specific strategy for an MDP quickly based on the learned meta policy}.
\noindent

\vspace{5mm}

\noindent\textbf{\large Research Methodology:}

\begin{enumerate}  \item \textbf{Algorithm Design:} Developing a Multi-Agent Meta-Reinforcement Learning algorithm using techniques such as \textbf{Multi-Agent Meta-Actor and Meta-Critic Networks}, with a focus on global optimization in MEC.\vspace{-1mm}

\item \textbf{Simulation Environment:} A simulated MEC environment will be developed using Python or a suitable simulation platform, where cload and edge servers be able to cache services and distribut tasks in whole resources, under different network conditions. 
	
\item \textbf{Key Challenges:}  (a) The meta-learned policy should work well across different, unseen tasks. (b) Balancing between exploration (learning new tasks) and exploitation (using learned knowledge). 
\end{enumerate}


\newpage



\begin{center} 
	
	
	\vspace{-17mm}
	
	\large Research Idea. 3  \hfill Federated Multi-Agent DRL \vspace{1mm} \hrule
	
	\vspace{-1mm}
	
	
	
	
	\textcolor{white}{i} \\ \LARGE Federated Multi-Agent Deep Reinforcement Learning for Continuous Improving Intradependente Task Offloading in Mobile Edge Computing Network\vspace{6mm}\\
	
\end{center}
\vspace{-5mm}

\noindent\textbf{\large Motivation:  } 
Federated Reinforcement Learning extends traditional DRL by allowing multiple agents to collaboratively learn a global policy without sharing their local data directly. Each agent makes decisions based on its local observations while cooperating with others to achieve shared system goals. In a Mobile Edge Computing (MEC) environment, edge devices can independently train DRL models using their local data and periodically send updates to a central server. The server aggregates these updates to build a global model, optimizing task offloading across the network.

%\noindent
%Federated DRL is an extension of DRL that allows multiple agents (or nodes) to collaboratively learn a global policy without sharing their local data. This is achieved by training local models on edge devices and sharing only model updates (e.g., gradients) with a central server to create a global model. The main aim is to enable decentralized training of a DRL model while preserving data privacy and reducing the need for central data storage. In MEC, each edge device could independently train a DRL model based on its local data, then periodically share updates with a central server that aggregates these updates to create a global model for optimized task offloading. 
%
%Combining deep reinforcement learning with federated learning yields federated deep reinforcement learning [10] as a new computing paradigm, where each user trains a local DQN and the center trains a general DQN. In each learning iteration, the center sends a policy to each user. By following this policy, each user performs an action and receives a reward. Each user then updates her local DQN using the received reward. Also, each user sends her reward signal back to the center which updates the general 
%DQN based on these reward signals.
%
%Moreover, multi-agent reinforcement learning (MARL) [18] and federated learning (FL) based reinforcement learning [19] are also developed for the distributed scenes where agents learn to make decisions through their local observations and cooperate for the same system targets. Motivated by these, we consider a multi-agent reinforcement learning approach for joint MEC collaboration to maintain the data freshness.


\vspace{3mm}

\noindent\textbf{\large Problem Statement: }


\vspace{0mm}
	\begin{itemize}
	\item \textbf{Dependency-Aware Task Partitioning}\vspace{-2mm}
	\begin{itemize}
		\item Incorporate a \textbf{Task Call Graph Representation} to account for dependencies among tasks, improving task model accuracy and partitioning effectiveness.
	\end{itemize}
\vspace{-3mm}
	\item \textbf{Federated Deep Reinforcement Learning}\vspace{-2mm}
	\begin{itemize}
		\item Leverage federated learning for mobile devices in training process. \vspace{-2mm}
		\item Enable mobile devices to collectively contribute to enhancing the offloading model.\vspace{-2mm}
		\item Support continuous learning as new mobile devices join the network.\vspace{-2mm}
	\end{itemize}
\end{itemize}

\vspace{3mm}

\noindent\textbf{\large Problem Model: } 
\noindent



\vspace{5mm}

\noindent\textbf{\large Research Methodology}

\begin{enumerate} \item \textbf{Algorithm Design:} Developing a federated deep reinforcement learning framework for optimizing interdependent task offloading in MEC networks, using teqnics such as DQN or DDPG, which enable network entities to collaboratively learn and adapt offloading strategies without compromising user privacy. \item \textbf{Simulation Environment:}  \item \textbf{Key Challenges:} (a) Communication efficiency, (b) Heterogeneity  
\end{enumerate}



\section{Summary of Differences} \begin{itemize} \item \textbf{Multi-Agent DRL} focuses on interactions between multiple agents, each learning in the presence of others. \item \textbf{Federated DRL} focuses on collaborative learning across decentralized devices while preserving data privacy. \item \textbf{Meta DRL} focuses on quick adaptation to new tasks by using past learning experiences. \end{itemize}




\bibliographystyle{IEEEtranN} % IEEEtranN is the natbib compatible bst file
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{paper}




\end{document}


