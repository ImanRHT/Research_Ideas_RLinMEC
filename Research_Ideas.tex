%\documentclass{article}
%\usepackage[letterpaper,margin=2.1cm]{geometry}
%\usepackage{xcolor}
%\usepackage{fancyhdr}
%\usepackage{tgschola} % or any other font package you like 

\documentclass[12pt]{article}
\usepackage{extsizes}
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{color}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsxtra}
\usepackage{wasysym}
\usepackage{isomath}
\usepackage{mathtools}
\usepackage{txfonts}
\usepackage{upgreek}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{tensor}
\usepackage{pifont}
\usepackage[margin=15mm]{geometry}
\definecolor{color-1}{rgb}{0.26,0.26,0.26}
\definecolor{color-2}{rgb}{0.4,0.4,0.4}
\usepackage{extsizes}
\usepackage{tocbibind}
\usepackage{float}
\usepackage{flafter}
\usepackage{xcolor}
\usepackage{sectsty}
\usepackage[font=small, skip=0pt]{caption}
\usepackage{setspace}
\setstretch{1.1}
\usepackage{fancyhdr}

\usepackage{nopageno}

% Select the font
\usepackage{charter}


\usepackage[%
square,        % for square brackets
comma,         % use commas as separators
numbers,       % for numerical citations;
%sort           % orders multiple citations into the sequence in which they appear in the list of references;
sort&compress % as sort but in addition multiple numerical citations
% are compressed if possible (as 3-6, 15);
]{natbib}

\renewcommand{\bibfont}{\normalfont\footnotesize}



\usepackage{hyperref}
\hypersetup{
	colorlinks = true,
	citecolor = {blue},
	linkcolor = {blue},
	urlcolor  = {blue},
}






\newcommand{\soptitle}{Intelligent Mobile Edge Computing Systems with Reinforcement Learning-Based Innovations}
\newcommand{\yourname}{Iman Rahmati}
\newcommand{\youremail}{iman.rahmati@sharif.edu}
\newcommand{\yourweb}{\href{https://imanrht.github.io}{imanrht.github.io}}

\newcommand{\statement}[1]{\par\medskip
	\underline{\textcolor{blue}{\textbf{#1:}}}\space
}

%\usepackage[
%colorlinks,
%breaklinks,
%pdftitle={\yourname - \soptitle},
%pdfauthor={\yourname},
%urlcolor  = blue,
%citecolor = blue,
%anchorcolor = blue,
%unicode
%]{hyperref}


\usepackage{setspace}
\onehalfspacing

\begin{document}
	

	
%\pagestyle{fancy}
%\fancyhf{}
%\fancyhead[C]{%
%	\footnotesize\sffamily\vspace{8mm}
%	\textcolor{blue}{\href{mailto:iman.rahmati@sharif.edu}{Research Ideas, V0.1}}  \hfill
%	\textcolor{blue}{\href{https://imanrht.github.io/assets/images/CV_ImanRahmati.pdf}{20 Sep. 2024\vspace{2mm}}}}
%

\begin{center} 
	
	\vspace{-17mm}
	
	\large Iman Rahmati\footnote{The author is with the EdgeAI Laboratory at the Department of Electrical Engineering, Sharif University of Technology, Tehran, Iran (\href{mailto:iman.rahmati@sharif.edu}{Mail}, \href{https://github.com/ImanRHT}{Github})} \hfill Research Ideas, 22 Sep. 2024 \vspace{1mm} \hrule
	
	\vspace{-5mm}
	
	
	\noindent\Large 
	\textcolor{white}{i} \\ \LARGE\soptitle \vspace{6mm}\\
	%\Large\yourname \large
\end{center}



%	\hrule
%	\vspace{0.5pt}
%	\hrule height 1pt



\vspace{-8mm}

\begin{abstract}
	\noindent
	Mobile edge computing often suffers from the dynamic and unknown nature of the environment, such as time-varying conditions, heterogeneous devices, and frequent communication requests, imposing significant challenges on improving system performance. To meet the rapidly growing demands of computation-intensive and time-sensitive applications, Reinforcement learning (RL)\cite{mnih2015human} has been proposed as an effective tool to establish low-latency and energy-efficient networks. RL enables network entities to interact with the environment and learn an optimal decision-making policy, usually modeled as a Markov decision process (MDP)\cite{puterman2014markov}.\vspace{2mm}
	
	\noindent\textit{\textbf{Termsâ€”}} Mobile edge computing (MEC), Resource Management, Computation Offloading, Partially Observable MDP (POMDP), Deep RL (DRL), Multi-Agent RL (MARL), Meta RL, Federated RL
\end{abstract}





\vspace{0mm}
\noindent\large\textbf{Introduction: }
\small
MEC is emerging as a promising paradigm to enhance the computational capacity of mobile devices by offloading tasks to nearby edge servers \cite{mao2017survey}. This paradigm aims to reduce latency, energy consumption, and improve quality of experience for end-users. However, one of the major challenges in MEC is the efficient decision-making process for resource management, considering the dynamic nature of the network, user demands, and limited resources. Traditional offloading strategies, which often rely on heuristic or single-agent models, fail to capture the complexity and stochastic nature of modern MEC. \\\vspace{-4mm}


\noindent\large\textbf{Background: }\small
To cope with the dynamic nature of the network, state-of-the-art research has proposed several task offloading algorithms using RL-based methods, which hold promises to determine optimal decision-making policies by capturing the dynamics of environments and learning strategies for accomplishing long-term objectives \cite{arulkumaran2017deep}. To minimize energy consumption, Munir \textit{et al.} \cite{munir2021multi} developed a semi-distributed approach using a multi-agent RL (MARL) framework for self-powered MEC. Gong \textit{et al.} in \cite{gong2022edge} proposed a DRL-based network structure in the industrial IoT (IIoT) systems to jointly optimize task offloading and resource allocation to achieve lower energy consumption and decreased task delay. Sun \textit{et al.} in \cite{sun2024hierarchical} explored both computation offloading and service caching problems in MEC. They proposed a hierarchical DRL framework, which effectively handles both problems under heterogeneous resources.


%Munir et al. introduced a semi-distributed MADRL method for self-powered MEC. Gong et al. proposed a DRL structure for task offloading and resource allocation, aiming to reduce energy use and task delays. Wu et al. addressed privacy in computation offloading using a deep Q-network (DQN) to optimize energy and computation rates in queuing systems. Lastly, Sun et al. tackled computation offloading and service caching with a hierarchical DRL framework to minimize long-term service delays across diverse resources.

Although DRL-based methods in some releated research like \cite{munir2021multi}--\cite{sun2024hierarchical} have demonstrated effectiveness in handling network dynamics, resource management still encounters several challenges that require further attention: \vspace{0mm}
\begin{enumerate}
	\item  Single-agent \textbf{non-stationarity} issues \cite{hernandez2017survey}, which motivate leveraging \textbf{MARL}. \vspace{-1.5mm}
	
	\item \textbf{Partially observable environment} issues, which motivate the use of decentralized \textbf{POMDP}.\vspace{-1.5mm}
	
	\item The \textbf{dynamic and heterogeneous nature of network} environments, motivates the use of \textbf{Meta RL}.\vspace{-1.5mm}
	
	\item \textbf{Decentralization of learning process}, where data privacy, transmission link, and computational resources are critical concerns, motivate leveraging \textbf{Federated RL}.
\end{enumerate}
 	
% 	\begin{enumerate}
% 		\item  Single-agent RL learns its decision-making policy independently and treats other agents as part of the environment, which may cause the non-stationarity issue \cite{hernandez2017survey} and significantly reduce learning efficiency and network performance, which motivate leveraging \textbf{multi-agent RL}
% 		\item The dynamic and heterogeneous nature of network environments, such as fluctuating network conditions, varying resource availability, and diverse user demands, motivates the use of \textbf{Meta RL}. This approach helps agents quickly adapt to new responsibilities with minimal data.
% 		
% 		\item\textbf{\hspace{2mm}Edge-edge task offloading.}
% 	\end{enumerate}



\newpage





\begin{center} 
	
	
	\vspace{-17mm}
	
	\large Research Idea. 1  \hfill Multi-Agent DRL \vspace{1mm} \hrule
	
	\vspace{-1mm}
	
	\textcolor{white}{i} \\ \LARGE Multi-Agent Deep Reinforcement Learning for Cooperative Task Offloading in Partially Observable Mobile Edge Computing Environment \vspace{6mm}\\ 
	
\end{center} \small
\vspace{-5mm}

\noindent\textbf{\large Motivation:  }
\noindent
In MEC, each entity may need to make local decisions to improve network performance in dynamic and uncertain environments. Standard learning algorithms, such as single-agent RL or DRL \cite{liao2023online}, \cite{huang2019deep}, have recently been used to enable each network entity to learn an optimal decision-making policy adaptively through interaction with the unknown environment. However, these algorithms fail to model cooperation or competition among network entities, treating other entities simply as part of the environment, which can lead to non-stationarity issues. MARL enables each network entity to learn its optimal policy by observing both the environment and the policies of other entities while interacting with a shared or separate environment to achieve specific objectives \cite{zhang2021multi}.

%As a result, MARL can significantly improve the learning efficiency of network entities, and it has been recently used to solve various issues in emerging networks.


%In multi-agent DRL, multiple agents interact with a shared or separate environment to achieve specific objectives. Each agent independently learns through trial and error while accounting for the actions and policies of other agents. Multiple agents interact with a shared or separate environment to achieve specific objectives. In mobile edge computing, each device might be an agent trying to optimize its computation offloading strategy while considering the resource usage and strategies of other devices.


\vspace{2mm}

\noindent\textbf{\large Problem Statement: }
\noindent
Task offloading is a critical process to efficiently assign available resources to task requests, for high-performance, reliable, and cost-effective services. In MEC, the decision-making process of task offloading focuses on efficiently distributing tasks among edge servers, where resources refer to limited computation, storage, and communication resources of edge and cloud servers. Typically, the offloading process involves two layers of heterogeneous decisions making problems (\textbf{P1, P2}) as follows,
%The task offloading decision-making process focuses on efficiently distributing tasks among edge servers. 
\vspace{-2mm}
\begin{itemize}
	\item\textbf{P1.\hspace{2mm}Devise-edge task offloading.} Enables devices to independently make decisions on offloading resource-intensive tasks to nearby edge servers, fostering efficient utilization of resources.\vspace{-2mm}
	\item\textbf{P2.\hspace{2mm}Edge-edge task offloading.} Leverages edge-edge collaborations, where tasks initially received by a local edge server can be offloaded to neighboring servers with underutilized resources. %Offloading tasks between edge servers require communication resources and may introduce additional transmission delay, which should be taken into account when designing offloading strategies.
\end{itemize}

\noindent\textbf{\large Problem Model: }
\noindent
The main problem can be formulated as the decomposition of sub-problems \textbf{P1} and \textbf{P2} as a \textbf{Decenteralized Partially Observable Markov Decision Processes (Dec-POMDP}) \cite{oliehoek2016concise}, where multiple devices and edge servers interacting with each other by its observation of the environment, which is a part of main overall state. 

\vspace{3mm}

\noindent\textbf{\large Research Methodology: }
\begin{enumerate} 
	\item \textbf{Algorithm Design:} Developing a \textbf{MARL} algorithm using techniques such as \textbf{Deep Deterministic Policy Gradient (DDPG)} \cite{lillicrap2015continuous} or \textbf{Dueling Double Deep Q-Networks (D3QN)} \cite{van2016deep}, with a focus on communication and collaboration, coordination or competition between agents. \vspace{-1mm}
	\item \textbf{Simulation Environment:} A simulated MEC environment will be developed using Python or a suitable simulation platform, where mobile devices can offload tasks to edge servers and edge servers can distribute their computation workloads, under different network conditions.\vspace{-1mm}
	\item \textbf{Key Challenges:} (a) Coordination or competition between agents. (b) The non-stationary environment due to actions of other agents.  (c) Scalability issues as the number of agents increases.
\end{enumerate}


\newpage




\begin{center} 
	
	
	\vspace{-17mm}
	
	\large Research Idea. 2  \hfill Meta RL \vspace{1mm} \hrule
	
	\vspace{-1mm}
	
	
	
	\textcolor{white}{i} \\ \LARGE Meta-Reinforcement Learning for Optimized Resource Management in Heterogeneous Mobile Edge Computing  \vspace{6mm}\\
	
\end{center}
\vspace{-5mm}

\noindent\textbf{\large Motivation:  }
\noindent
Meta DRL focuses on training agents that can quickly adapt to new tasks or environments with minimal additional learning \cite{beck2023survey}. It is designed for scenarios where agents face a wide variety of tasks, and the aim is to learn a policy that generalizes well across different tasks. The primary objective is to equip the agent with meta-knowledge, allowing it to efficiently adapt to new tasks by leveraging past learning experiences. In MEC, a meta-trained agent could adapt its offloading strategy efficiently when moving between different environments, quickly optimizing its offloading decisions in unfamiliar settings.


\vspace{3mm}



\noindent\textbf{\large Problem Statement: } Efficient task offloading is crucial to ensure seamless resource distribution in MEC.
Typically, the overall Resource Management process involves three layers of heterogeneous Resource scheduling decisions (\textbf{P1}, \textbf{P2}, \textbf{P3}), each of which performs in a specific collaboration manner. \vspace{-2mm}

\begin{itemize}
	
	\item \textbf{P1. Edge-cloud service placement} \cite{farhadi2021service}.  %In MEC systems, clouds have sufficient resources, while edge servers are resource-limited. 
	The cloud caches all services with sufficient storage space. Considering the storage limits of edge servers, only a subset of services can be placed in each edge server. Services can be migrated from a cloud to an edge or between edge servers, which requires efficient collaboration.\vspace{-2mm}
	
	\item\textbf{P2. Edge-edge computation offloading} \cite{han2022edgetuner}. The task offloading decision-making process focuses on efficiently distributing tasks among edge servers. Edge-edge collaborations enable edge servers to offload their computation workload to neighboring servers, ensuring better resource utilization. \vspace{-2mm}
	
	\item\textbf{P3. Intra-edge resource allocation} \cite{xiong2020resource}. On edge servers, there may be several tasks competing for resources among offloaded tasks on the same server. Intra edge there is a resource competition among offloaded tasks on the same server. Intra-edge resource allocation aims to determine how resources should be allocated to each offloaded task.
	
\end{itemize}

\vspace{0mm}

\noindent\textbf{\large Problem Model: } To apply Meta RL for address combination of sub-problems  \textbf{P1},  \textbf{P2}, and \textbf{P3}, each problem can be formulated as an individual MDP model. The MDP learning process should be decomposed into two parts: \textbf{learning a meta policy efficiently across all MDPs} and \textbf{learning a specific strategy for an MDP quickly based on the learned meta policy}.
\noindent

\vspace{5mm}

\noindent\textbf{\large Research Methodology:}

\begin{enumerate}  \item \textbf{Algorithm Design:} Developing a Multi-Agent Meta-Reinforcement Learning algorithm using techniques such as \textbf{Meta-Actor and Meta-Critic Networks} \cite{ding2023multiagent}, with a focus on global optimization in MEC.\vspace{-1mm}
	
	\item \textbf{Simulation Environment:} A simulated MEC environment will be developed using Python or a suitable simulation platform, where cloud and edge servers be able to cache services and distribute tasks in whole resources, under different network conditions. 
	
	\item \textbf{Key Challenges:}  (a) The meta-learned policy should work well across different, unseen tasks. (b) Balancing between exploration (learning new tasks) and exploitation (using learned knowledge). 
\end{enumerate}


\newpage



\begin{center} 
	
	
	\vspace{-17mm}
	
	\large Research Idea. 3  \hfill Federated DRL \vspace{1mm} \hrule
	
	\vspace{-1mm}
	
	
	
	
	\textcolor{white}{i} \\ \LARGE Federated Reinforcement Learning for Dependency-Aware Task Offloading in Mobile Edge Computing Network\vspace{6mm}\\
	
\end{center}
\vspace{-5mm}

\noindent\textbf{\large Motivation:  } 
Federated Reinforcement Learning extends traditional DRL by allowing multiple agents to collaboratively learn a global policy without sharing their local data directly. Each agent makes decisions based on its local observations while cooperating with others to achieve shared system goals. In an MEC environment, edge devices can independently train DRL models using their local data and periodically send updates to a central server. The server aggregates these updates to build a global model, optimizing task offloading across the network. Specifically, federated DRL focuses on collaborative learning across decentralized devices while preserving data privacy \cite{lim2020federated}, some advantage of federated RL involve: 
\vspace{-2mm}
\begin{itemize}
	
	\item \textbf{Accelerate training process} for agents involved in MEC network. \vspace{-2mm}
	\item Enable mobile devices to  \textbf{collectively contribute to enhancing the offloading model}.\vspace{-2mm}
	\item Support  \textbf{continuous learning} as new mobile devices join the network.\vspace{-2mm}
\end{itemize}

\vspace{3mm}

\noindent\textbf{\large Problem Statement: }
Efficient task offloading is crucial for optimizing resource utilization and minimizing latency. However, existing approaches often treat tasks as independent, overlooking the complexities introduced by intra-dependencies among tasks, leading to suboptimal resource utilization and performance. To address dependency-aware task offloading, there is a need to effectively model intra-dependent tasks,
\vspace{-1mm}

	\begin{itemize}
		\item Incorporate a \textbf{Task Call Graph Representation} \cite{feng2024dependency} to account for dependencies among tasks, improving task model accuracy and offloading effectiveness. Task call graphs can effectively represent dependencies, allowing for a clearer understanding of task relationships. 
	\end{itemize}\vspace{-3mm}
	
	


\vspace{3mm}

\noindent\textbf{\large Problem Model: } The problem can be formulated as MDPs, where multiple devices and edge servers interact with each other by their observation of the environment and learn global policies to achieve shared system goals.
\noindent



\vspace{5mm}

\noindent\textbf{\large Research Methodology}

\begin{enumerate} \item \textbf{Algorithm Design:} Developing a  \textbf{federated DRL} framework for optimizing interdependent task offloading in MEC networks, using technics \textbf{federated Actor-Critic Network} \cite{zhu2021federated} or \textbf{federated D3QN} \cite{nguyen2021federated}, which enable network entities to collaboratively learn and adapt offloading strategies without compromising user privacy. \item \textbf{Simulation Environment:} A simulated MEC environment will be developed using Python or a suitable simulation platform, where devices be able to dispatch their tasks to edge servers and provide collaboration for all devices and edge servers, under different network conditions. 
	
	
	\item \textbf{Key Challenges:} (a) Communication efficiency to optimize data exchange between devices and servers.  (b) Heterogeneity to accommodate diverse network components and devices.
\end{enumerate}




\setstretch{0.8}

\bibliographystyle{IEEEtranN}% IEEEtranN is the natbib compatible bst file
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{paper}




\end{document}


